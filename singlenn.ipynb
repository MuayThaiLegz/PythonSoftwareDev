{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "The learning objectives of this assignment are to:\n",
    "1. implement feed-forward prediction for a single layer neural network \n",
    "2. implement training via back-propagation for a single layer neural network \n",
    "\n",
    "# Setup your environment\n",
    "\n",
    "You will need to set up an appropriate coding environment on whatever computer\n",
    "you expect to use for this assignment.\n",
    "Minimally, you should install:\n",
    "\n",
    "* [git](https://git-scm.com/downloads)\n",
    "* [Python (version 3.8 or higher)](https://www.python.org/downloads/)\n",
    "* [numpy](http://www.numpy.org/)\n",
    "* [pytest](https://docs.pytest.org/)\n",
    "* [pytest-timeout](https://pypi.org/project/pytest-timeout/)\n",
    "\n",
    "# Check out the starter code\n",
    "\n",
    "After accepting the assignment on GitHub Classroom, clone the newly created\n",
    "repository to your local machine:\n",
    "```\n",
    "git clone https://github.com/ua-ista-457/back-propagation-<your-username>.git\n",
    "```\n",
    "You are now ready to begin working on the assignment.\n",
    "You should do all your work in the default branch, `main`.\n",
    "\n",
    "# Write your code\n",
    "\n",
    "You will implement a simple single-layer neural network with sigmoid activations\n",
    "everywhere.\n",
    "This will include making predictions with a network via forward-propagation, and\n",
    "training the network via gradient descent, with gradients calculated using\n",
    "back-propagation.\n",
    "\n",
    "You should read the documentation strings (docstrings) in each of methods in\n",
    "`nn.py`, and implement the methods as described.\n",
    "Write your code below the docstring of each method;\n",
    "**do not delete the docstrings**.\n",
    "\n",
    "The following objects and functions may come in handy:\n",
    "* [numpy.ndarray.dot](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.dot.html)\n",
    "* [numpy.ndarray.T](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html)\n",
    "* [numpy.where](https://numpy.org/doc/stable/reference/generated/numpy.where.html)\n",
    "* [scipy.special.expit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)\n",
    "\n",
    "# Test your code for correctness\n",
    "\n",
    "The tests in `test_nn.py` check that each method behaves as expected.\n",
    "To run all the provided tests, run ``pytest`` from the directory containing\n",
    "``test_nn.py``.\n",
    "Initially, you will see output like:\n",
    "```\n",
    "============================= test session starts ==============================\n",
    "...\n",
    "collected 5 items\n",
    "\n",
    "test_nn.py FFFFF                                                         [100%]\n",
    "\n",
    "=================================== FAILURES ===================================\n",
    "...\n",
    "============================== 5 failed in 0.65s ===============================\n",
    "```\n",
    "This indicates that all tests are failing, which is expected since you have not\n",
    "yet written the code for any of the methods.\n",
    "Once you have written the code for all methods, you should instead see\n",
    "something like:\n",
    "```\n",
    "============================= test session starts ==============================\n",
    "...\n",
    "collected 5 items\n",
    "\n",
    "test_nn.py .....                                                         [100%]\n",
    "\n",
    "============================== 5 passed in 0.47s ===============================\n",
    "```\n",
    "\n",
    "# Test your code for quality\n",
    "\n",
    "In addition to the correctness tests, you should run `pylint nn.py` to check\n",
    "for common code quality problems.\n",
    "Pylint will check for adherence to\n",
    "[standard Python style](https://www.python.org/dev/peps/pep-0008/),\n",
    "good variable names, proper use of builtins like `enumerate`, etc.\n",
    "If you use `scipy.special.expit`, you may also add the\n",
    "`--extension-pkg-allow-list=scipy.special` option to silence a lint warning\n",
    "from that package.\n",
    "If there are no problems, you should see something like:\n",
    "```\n",
    "--------------------------------------------------------------------\n",
    "Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)\n",
    "```\n",
    "\n",
    "# Submit your code\n",
    "\n",
    "As you are working on the code, you should regularly `git commit` to save your\n",
    "current changes locally.\n",
    "You should also regularly `git push` to push all saved changes to the remote\n",
    "repository on GitHub.\n",
    "Make a habit of checking the \"Feedback\" pull request on the GitHub page for your\n",
    "repository.\n",
    "You should see all your pushed commits there, as well as the status of the\n",
    "\"checks\".\n",
    "If any correctness (pytest) or quality (pylint) tests are failing, you will see\n",
    "\"All checks have failed\" at the bottom of the pull request.\n",
    "If you want to see exactly which tests have failed, click on the \"Details\" link.\n",
    "When you have corrected all problems, you should see \"All checks have passed\"\n",
    "at the bottom of the pull request.\n",
    "\n",
    "You do not need to do anything beyond pushing your commits to Github to submit\n",
    "your assignment.\n",
    "The instructional team will grade the code of the \"Feedback\" pull request, and\n",
    "make detailed comments there.\n",
    "\n",
    "# Grading\n",
    "\n",
    "The points are allocated as follows:\n",
    "* 80 points for passing all automated correctness (pytest) tests\n",
    "* 10 points for passing all automated quality (pylint) tests\n",
    "* 10 points for other quality issues:\n",
    "using appropriate data structures,\n",
    "using existing library functions whenever appropriate,\n",
    "minimizing code duplication,\n",
    "giving variables meaningful names,\n",
    "documenting complex pieces of code, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_structure(X, Y):\n",
    "    input_unit = X.shape[0] # size of input layer\n",
    "    hidden_unit = 4 #hidden layer of size 4\n",
    "    output_unit = Y.shape[0] # size of output layer\n",
    "    return (input_unit, hidden_unit, output_unit)\n",
    "\n",
    "(input_unit, hidden_unit, output_unit) = define_structure(np.asarray(X_train), np.asarray(y_train))\n",
    "print(\"The size of the input layer is:  = \" + str(input_unit))\n",
    "print(\"The size of the hidden layer is:  = \" + str(hidden_unit))\n",
    "print(\"The size of the output layer is:  = \" + str(output_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[0.00786466]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 31 19:27:33 2017\n",
    "@author: rodol\n",
    "\"\"\"\n",
    "from numpy import exp, array, random, dot\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We model a single neuron, with 3 input connections and 1 output connection.\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a single neuron neural network.\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print (\"Random starting synaptic weights: \")\n",
    "    print (neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "    # Train the neural network using a training set.\n",
    "    # Do it 10,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print (\"New synaptic weights after training: \")\n",
    "    print (neural_network.synaptic_weights)\n",
    "\n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print (neural_network.think(array([0, 1, 1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class FCLayer:\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    @classmethod\n",
    "    def Random(cls, *layer_units: int):\n",
    "\n",
    "        def uniform(n_in, n_out):\n",
    "            epsilon = math.sqrt(6) / math.sqrt(n_in + n_out)\n",
    "            return np.random.uniform(-epsilon, +epsilon, size=(n_in, n_out))\n",
    "\n",
    "        pairs = zip(layer_units, layer_units[1:])\n",
    "        return cls(*[uniform(i, o) for i, o in pairs])\n",
    "    \n",
    "\n",
    "    def __init__(self, *layer_weights: np.ndarray):\n",
    "        random.seed(1)\n",
    "\n",
    "        \n",
    "        self.results = []\n",
    "                \n",
    "        self.weights = layer_weights\n",
    "        self.bias = None #np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) #+ self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "            # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            #for layer in :\n",
    "            output = self.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = FCLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,3) and (0,) not aligned: 3 (dim 1) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m layer\u001b[39m.\u001b[39;49mforward_propagation(training_set_inputs)\n",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 8\u001b[0m in \u001b[0;36mFCLayer.forward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_propagation\u001b[39m(\u001b[39mself\u001b[39m, input_data):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X40sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m input_data\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights) \u001b[39m#+ self.bias\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,3) and (0,) not aligned: 3 (dim 1) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "layer.forward_propagation(training_set_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,3) and (0,) not aligned: 3 (dim 1) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m layer\u001b[39m.\u001b[39;49mbackward_propagation(training_set_inputs, \u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 8\u001b[0m in \u001b[0;36mFCLayer.backward_propagation\u001b[1;34m(self, output_error, learning_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_propagation\u001b[39m(\u001b[39mself\u001b[39m, output_error, learning_rate):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     input_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(output_error, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     weights_error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput\u001b[39m.\u001b[39mT, output_error)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# dBias = output_error\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#X36sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# update parameters\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,3) and (0,) not aligned: 3 (dim 1) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "layer.backward_propagation(training_set_inputs, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation functions\n",
    "\n",
    "def linear_forward(value, weight):\n",
    "    return np.matmul(value, weight.T)\n",
    "\n",
    "\n",
    "def sigmoid_forward(value):\n",
    "    return 1 / (1 + np.exp(-value))\n",
    "\n",
    "\n",
    "def softmax_forward(trial):\n",
    "    # return np.array([[np.exp(value)/np.sum(np.exp(row)) for value in row] for row in trial])\n",
    "    return np.array([np.exp(value) / np.sum(np.exp(trial)) for value in trial])\n",
    "\n",
    "\n",
    "def cross_entropy_forward(y_predicted, y_actual):\n",
    "    # return np.sum(y_actual*np.log(y_predicted))/ y_predicted.shape[0]\n",
    "    return - np.sum(y_actual * np.log(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleNetwork:\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls, *layer_units: int):\n",
    "\n",
    "        def uniform(n_in, n_out):\n",
    "            epsilon = math.sqrt(6) / math.sqrt(n_in + n_out)\n",
    "            return np.random.uniform(-epsilon, +epsilon, size=(n_in, n_out))\n",
    "\n",
    "        pairs = zip(layer_units, layer_units[1:])\n",
    "        return cls(*[uniform(i, o) for i, o in pairs])\n",
    "\n",
    "    def __init__(self, *layer_weights: np.ndarray):\n",
    "           \n",
    "        \n",
    "    def predict(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    def predict_zero_one(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        \n",
    "    def gradients(self,\n",
    "                  input_matrix: np.ndarray,\n",
    "                  output_matrix: np.ndarray) -> List[np.ndarray]:\n",
    "      \n",
    "    def train(self,\n",
    "              input_matrix: np.ndarray,\n",
    "              output_matrix: np.ndarray,\n",
    "              iterations: int = 10,\n",
    "              learning_rate: float = 0.1) -> None:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNetwork:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    cost_ : list\n",
    "      Sum-of-squares cost function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def Random(cls, *layer_units: int):\n",
    "\n",
    "        def uniform(n_in, n_out):\n",
    "            epsilon = math.sqrt(6) / math.sqrt(n_in + n_out)\n",
    "            return np.random.uniform(-epsilon, +epsilon, size=(n_in, n_out))\n",
    "\n",
    "        pairs = zip(layer_units, layer_units[1:])\n",
    "        return cls(*[uniform(i, o) for i, o in pairs])\n",
    "\n",
    "    def __init__(self, *layer_weights: np.ndarray):\n",
    "        \n",
    "        self.layer_weights = layer_weights\n",
    "        self.results = []\n",
    "        self.eta=0.01\n",
    "        self.n_iter = 10\n",
    "    def fit(self, X, y):\n",
    "    \n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            net_input = self.net_input(X)\n",
    "            # Please note that the \"activation\" method has no effect\n",
    "            # in the code since it is simply an identity function. We\n",
    "            # could write `output = self.net_input(X)` directly instead.\n",
    "            # The purpose of the activation is more conceptual, i.e.,  \n",
    "            # in the case of logistic regression (as we will see later), \n",
    "            # we could change it to\n",
    "            # a sigmoid function to implement a logistic regression classifier.\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "    \n",
    "    \n",
    "    def gradients(self, X, y):\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        \n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \n",
    "        def sigmoid(X):\n",
    "            \"\"\"The sigmoid function.\"\"\"\n",
    "            return 1.0/(1.0+np.exp(-X))\n",
    "\n",
    "        def sigmoid_prime(X):\n",
    "            \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "            return sigmoid(X)*(1-sigmoid(X))\n",
    "            \"\"\"Compute linear activation\"\"\"\n",
    "            \n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights at the start of training\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New weights after training\n",
      "[[5.39428067]\n",
      " [0.19482422]\n",
      " [0.34317086]]\n",
      "Testing network on new examples ->\n",
      "[0.99995873]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights at the start of training\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sigmoid() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m train_inputs \u001b[39m=\u001b[39m array([[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m train_outputs \u001b[39m=\u001b[39m array([[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mT\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m neural_network\u001b[39m.\u001b[39;49mtrain(train_inputs, train_outputs, \u001b[39m10000\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mNew weights after training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m (neural_network\u001b[39m.\u001b[39mweight_matrix)\n",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 18\u001b[0m in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, train_inputs, train_outputs, num_train_iterations)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_inputs, train_outputs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                         num_train_iterations):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                              \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# Number of iterations we want to\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# perform for this set of input.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_train_iterations):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_propagation(train_inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39m# Calculate the error in the output.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         error \u001b[39m=\u001b[39m train_outputs \u001b[39m-\u001b[39m output\n",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\TutorSection\\singlenn.ipynb Cell 18\u001b[0m in \u001b[0;36mNeuralNetwork.forward_propagation\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_propagation\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/TutorSection/singlenn.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigmoid(dot(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_matrix))\n",
      "\u001b[1;31mTypeError\u001b[0m: sigmoid() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Python program to implement a\n",
    "# single neuron neural network\n",
    " \n",
    "# import all necessary libraries\n",
    "from numpy import exp, array, random, dot, tanh\n",
    " \n",
    "# Class to create a neural\n",
    "# network with single neuron\n",
    "class NeuralNetwork():\n",
    "     \n",
    "    def __init__(self):\n",
    "         \n",
    "        # Using seed to make sure it'll \n",
    "        # generate same weights in every run\n",
    "        random.seed(1)\n",
    "         \n",
    "        # 3x1 Weight matrix\n",
    "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    "        self.results = []\n",
    "\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "            return 1 / (1 + exp(-x))\n",
    "\n",
    "        # The derivative of the Sigmoid function.\n",
    "        # This is the gradient of the Sigmoid curve.\n",
    "        # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, input_matrix: np.ndarray):\n",
    "        \n",
    "        for i in range(self.NumOfSamples):\n",
    "            # forward propagation\n",
    "            output = input_matrix[i]\n",
    "            #for layer in self.layers:\n",
    "            output = self.gradients(input_matrix)\n",
    "            self.results.append(output)\n",
    "            \n",
    "\n",
    "    def gradients(self, X, y):\n",
    "            output = self.__sigmoid(dot(X, self.weight_matrix))\n",
    "            error = training_set_outputs - output\n",
    "            \n",
    "            adjustment = np.matmul(training_set_inputs, error * self.__sigmoid_derivative(output))\n",
    "            \n",
    "            self.weight_matrix += adjustment\n",
    "            \n",
    "        \n",
    "     \n",
    "    # training the neural network.\n",
    "    def train(self, train_inputs, train_outputs,\n",
    "                            num_train_iterations, learn_rate):\n",
    "                                 \n",
    "        # Number of iterations we want to\n",
    "        # perform for this set of input.\n",
    "        for iteration in range(num_train_iterations):\n",
    "            output = self.forward_propagation(train_inputs)\n",
    " \n",
    "            # Calculate the error in the output.\n",
    "            error = train_outputs - output\n",
    " \n",
    "            # multiply the error by input and then\n",
    "            # by gradient of tanh function to calculate\n",
    "            # the adjustment needs to be made in weights\n",
    "            adjustment = dot(train_inputs.T, error *\n",
    "                             self.sigmoid_prime(output))\n",
    "                              \n",
    "            # Adjust the weight matrix\n",
    "            self.weight_matrix += adjustment\n",
    " \n",
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    neural_network = NeuralNetwork()\n",
    "     \n",
    "    print ('Random weights at the start of training')\n",
    "    print (neural_network.weight_matrix)\n",
    " \n",
    "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    train_outputs = array([[0, 1, 1, 0]]).T\n",
    " \n",
    "    neural_network.train(train_inputs, train_outputs, 10000)\n",
    " \n",
    "    print ('New weights after training')\n",
    "    print (neural_network.weight_matrix)\n",
    " \n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Testing network on new examples ->\")\n",
    "    print (neural_network.forward_propagation(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('blockchainDev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e9c44cc3bc95fb73b0aae724063041b6d6060f5ce93be071123fdfeed5e731e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
