{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis.sklearn import prepare\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from termcolor import colored\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import re\n",
    "\n",
    "from random import randint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Dot, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('results')\n",
    "model_path = Path('results', 'bbc')\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  Path('./data/bbc')\n",
    "files = sorted(list(path.glob('**/*.txt')))\n",
    "doc_list = []\n",
    "for i, file in enumerate(files):\n",
    "    with open(str(file), encoding='latin1') as f:\n",
    "        topic = file.parts[-2]\n",
    "        lines = f.readlines()\n",
    "        heading = lines[0].strip()\n",
    "        body = ' '.join([l.strip() for l in lines[1:]])\n",
    "        doc_list.append([topic.capitalize(), heading, body])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBarr\\AppData\\Local\\Temp\\ipykernel_129156\\1404930559.py:9: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  stop_words = pd.read_csv('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words',\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "topic_labels = ['Topic {}'.format(i) for i in range(1, n_components + 1)]\n",
    "\n",
    "max_df = .2\n",
    "min_df = 3\n",
    "max_features = 2000\n",
    "\n",
    "# used by sklearn: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "stop_words = pd.read_csv('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words', \n",
    "                         header=None, \n",
    "                         squeeze=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results\\\\bbc\\\\lda_10_iter.pkl']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(doc_list, columns=['Category', 'Heading', 'Article'])\n",
    "\n",
    "\n",
    "train_docs, test_docs = train_test_split(docs,\n",
    "                                         stratify=docs.Category,\n",
    "                                         test_size= .5,\n",
    "                                         random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "countvectorizer = CountVectorizer(max_df=max_df, \n",
    "                             min_df=min_df, \n",
    "                             stop_words=stop_words, \n",
    "                             max_features=max_features)\n",
    "\n",
    "train_dtm = countvectorizer.fit_transform(train_docs.Article)\n",
    "test_dtm = countvectorizer.transform(test_docs.Article)\n",
    "\n",
    "\n",
    "lda_base = LatentDirichletAllocation(n_components=n_components,\n",
    "                                     n_jobs=-1,\n",
    "                                     learning_method='batch',\n",
    "                                     max_iter=10)\n",
    "lda_base.fit(train_dtm)\n",
    "\n",
    "joblib.dump(lda_base, model_path / 'lda_10_iter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "model_path = Path('movieclassifier', 'pkl_objects')\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_base = joblib.load(model_path / 'lda_10_iter.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'tokenizer' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\FamiliarFinanceAI\\Ai\\nlpForpay\\NewlayerTwo.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m objPath \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39mpkl_objects\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier_base \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(objPath\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclassifier.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m classifier_base\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[0;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    507\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1213\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\pickle.py:1537\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(name) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mstr\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mtype\u001b[39m(module) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mstr\u001b[39m:\n\u001b[0;32m   1536\u001b[0m     \u001b[39mraise\u001b[39;00m UnpicklingError(\u001b[39m\"\u001b[39m\u001b[39mSTACK_GLOBAL requires str\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name))\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\pickle.py:1581\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \u001b[39m__import__\u001b[39m(module, level\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m-> 1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39;49mmodules[module], name)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1582\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(sys\u001b[39m.\u001b[39mmodules[module], name)\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[1;34m(obj, name)\u001b[0m\n\u001b[0;32m    329\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, subpath)\n\u001b[0;32m    330\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt get attribute \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m                              \u001b[39m.\u001b[39mformat(name, obj)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    333\u001b[0m \u001b[39mreturn\u001b[39;00m obj, parent\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'tokenizer' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "objPath = Path('pkl_objects')\n",
    "classifier_base = joblib.load(objPath/'classifier.pkl') \n",
    "classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Tfidfvectorizer = TfidfVectorizer(max_df=.25,\n",
    "                             min_df=.01,\n",
    "                             stop_words='english',\n",
    "                             binary=False)\n",
    "train_dtmTfidfv = Tfidfvectorizer.fit_transform(train_docs.Article)\n",
    "\n",
    "test_dtmTfidfv = Tfidfvectorizer.transform(test_docs.Article)\n",
    "train_token_countTfidfv = train_dtmTfidfv.sum(0).A.squeeze()\n",
    "Tfidfvtokens = Tfidfvectorizer.get_feature_names()\n",
    "Tfidfvword_count = pd.Series(train_token_countTfidfv,\n",
    "                       index=Tfidfvtokens).sort_values(ascending=False)\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, n_iter=5, random_state=42)\n",
    "svd.fit(train_dtmTfidfv)\n",
    "  \n",
    "train_doc_topicsTfidfv = svd.transform(train_dtmTfidfv)\n",
    "\n",
    "train_resultTfidfv = pd.DataFrame(data=train_doc_topicsTfidfv,\n",
    "                            columns=topic_labels,\n",
    "                            index=train_docs.Category)\n",
    "\n",
    "\n",
    "topics = pd.DataFrame(svd.components_.T, index=Tfidfvtokens, columns=topic_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evalTfidfv = pd.DataFrame(data=svd.transform(test_dtmTfidfv),\n",
    "                         columns=topic_labels,\n",
    "                         index=test_docs.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>0.166735</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>-0.089148</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>-0.129305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0.247760</td>\n",
       "      <td>-0.287505</td>\n",
       "      <td>0.159446</td>\n",
       "      <td>-0.167504</td>\n",
       "      <td>0.010546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>0.151718</td>\n",
       "      <td>0.073303</td>\n",
       "      <td>-0.030046</td>\n",
       "      <td>-0.040677</td>\n",
       "      <td>0.100706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>0.136796</td>\n",
       "      <td>0.048634</td>\n",
       "      <td>-0.166585</td>\n",
       "      <td>-0.048980</td>\n",
       "      <td>-0.241851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>0.142048</td>\n",
       "      <td>0.073519</td>\n",
       "      <td>0.070498</td>\n",
       "      <td>0.013955</td>\n",
       "      <td>-0.036789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0.167124</td>\n",
       "      <td>-0.135201</td>\n",
       "      <td>0.047897</td>\n",
       "      <td>-0.099982</td>\n",
       "      <td>-0.026431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>0.162986</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>0.039488</td>\n",
       "      <td>0.012043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>0.168069</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>-0.086423</td>\n",
       "      <td>-0.037452</td>\n",
       "      <td>0.077776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>0.197818</td>\n",
       "      <td>-0.018953</td>\n",
       "      <td>-0.140240</td>\n",
       "      <td>-0.037115</td>\n",
       "      <td>-0.098134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>0.162103</td>\n",
       "      <td>-0.080983</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>0.160724</td>\n",
       "      <td>0.012585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1113 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic 1   Topic 2   Topic 3   Topic 4   Topic 5\n",
       "Category                                                       \n",
       "Tech           0.166735  0.011555 -0.089148 -0.010527 -0.129305\n",
       "Sport          0.247760 -0.287505  0.159446 -0.167504  0.010546\n",
       "Business       0.151718  0.073303 -0.030046 -0.040677  0.100706\n",
       "Tech           0.136796  0.048634 -0.166585 -0.048980 -0.241851\n",
       "Politics       0.142048  0.073519  0.070498  0.013955 -0.036789\n",
       "...                 ...       ...       ...       ...       ...\n",
       "Sport          0.167124 -0.135201  0.047897 -0.099982 -0.026431\n",
       "Entertainment  0.162986  0.005018 -0.000710  0.039488  0.012043\n",
       "Business       0.168069  0.087723 -0.086423 -0.037452  0.077776\n",
       "Tech           0.197818 -0.018953 -0.140240 -0.037115 -0.098134\n",
       "Entertainment  0.162103 -0.080983 -0.006763  0.160724  0.012585\n",
       "\n",
       "[1113 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evalTfidfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\ndisplay(Tfidfvword_count.head(10), topics.loc[Tfidfvword_count.head(10).index],\\n)\\n\\n\\n\\n\\ntop_words, top_vals = pd.DataFrame(), pd.DataFrame()\\nfor topic, words_ in topics.items():\\n    top10 = words_.abs().nlargest(10).index\\n    vals = words_.loc[top10].values\\n    top_vals[topic] = vals\\n    top_words[topic] = top10.tolist()\\nsns.heatmap(pd.DataFrame(top_vals),\\n            annot=top_words,\\n            fmt='',\\n            center=0,\\n            cmap=sns.diverging_palette(0, 255, sep=1, n=256),\\n            ax=ax)\\nax.set_title('Top Words per Topic')\\nsns.despine()\\nfig.tight_layout()\\n\\ntrain_resultTfidfv.groupby(level='Category').mean().plot.bar(figsize=(14, 5), rot=0)\\n\\nsns.set(font_scale=1.3)\\nresultTfidfv = pd.melt(test_evalTfidfv.assign(Data='Train').append(\\n    test_evalTfidfv.assign(Data='Test')).reset_index(),\\n                 id_vars=['Data', 'Category'],\\n                 var_name='Topic',\\n                 value_name='Weight')\\n\\ng = sns.catplot(x='Category',\\n                y='Weight',\\n                hue='Topic',\\n                row='Data',\\n                kind='bar',\\n                data=resultTfidfv,\\n                aspect=3.5); \\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "display(Tfidfvword_count.head(10), topics.loc[Tfidfvword_count.head(10).index],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_words, top_vals = pd.DataFrame(), pd.DataFrame()\n",
    "for topic, words_ in topics.items():\n",
    "    top10 = words_.abs().nlargest(10).index\n",
    "    vals = words_.loc[top10].values\n",
    "    top_vals[topic] = vals\n",
    "    top_words[topic] = top10.tolist()\n",
    "sns.heatmap(pd.DataFrame(top_vals),\n",
    "            annot=top_words,\n",
    "            fmt='',\n",
    "            center=0,\n",
    "            cmap=sns.diverging_palette(0, 255, sep=1, n=256),\n",
    "            ax=ax)\n",
    "ax.set_title('Top Words per Topic')\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "\n",
    "train_resultTfidfv.groupby(level='Category').mean().plot.bar(figsize=(14, 5), rot=0)\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "resultTfidfv = pd.melt(test_evalTfidfv.assign(Data='Train').append(\n",
    "    test_evalTfidfv.assign(Data='Test')).reset_index(),\n",
    "                 id_vars=['Data', 'Category'],\n",
    "                 var_name='Topic',\n",
    "                 value_name='Weight')\n",
    "\n",
    "g = sns.catplot(x='Category',\n",
    "                y='Weight',\n",
    "                hue='Topic',\n",
    "                row='Data',\n",
    "                kind='bar',\n",
    "                data=resultTfidfv,\n",
    "                aspect=3.5); \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\FamiliarFinanceAI\\Ai\\nlpForpay\\NewlayerTwo.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(dest):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(dest)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m classifier_base \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mclassifier.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/FamiliarFinanceAI/Ai/nlpForpay/NewlayerTwo.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m classifier_base\n",
      "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classifier.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "classifier_base = joblib.load('classifier.pkl') \n",
    "classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('blockchainDev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e9c44cc3bc95fb73b0aae724063041b6d6060f5ce93be071123fdfeed5e731e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
